{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "Bo-hK9yl-Lbw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B5N4HfQGuNWo"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Retrieve the token from Colab's secrets\n",
        "hf_api_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# Check if the token was successfully retrieved\n",
        "if hf_api_token is None:\n",
        "    print(\"ERROR: HF_TOKEN was NOT found in Colab Secrets.\")\n",
        "    print(\"Please go to the 'Secrets' tab (key icon on left), ensure 'HF_TOKEN' is present,\")\n",
        "    print(\"its value is correct, and 'Notebook access' is ON. Then, restart the runtime.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "KFXZDVfKT1EI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_MODEL_ID = \"deepseek-ai/DeepSeek-R1-0528\"\n",
        "HF_PROVIDER = \"together\"\n",
        "\n",
        "# 4. Initialize the Hugging Face Inference Client with the specific provider\n",
        "try:\n",
        "    client = InferenceClient(\n",
        "        token=hf_api_token,    # Pass token\n",
        "        # model=HF_MODEL_ID\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not initialize Hugging Face InferenceClient: {e}\")\n",
        "    print(\"Please check your HF_TOKEN and provider name.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "rVZvPubHX1tU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Function to get chat completion\n",
        "def get_deepseek_completion(prompt_text, model_id=HF_MODEL_ID, temperature=0.7, max_tokens=150, top_p=1.0):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the MiniMaxAI/MiniMax-M1-80k model via its chat completion API.\n",
        "    Parameters:\n",
        "        prompt_text (str): The user's input message.\n",
        "        model_id (str): The ID of the model to use (defaults to MiniMaxAI/MiniMax-M1-80k).\n",
        "        temperature (float): Controls randomness (0.0 to 1.0).\n",
        "        max_tokens (int): Maximum number of tokens to generate (equivalent to max_new_tokens).\n",
        "        top_p (float): Nucleus sampling parameter (0.0 to 1.0).\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        # You can add a system message here if the model benefits from it.\n",
        "        # {\"role\": \"system\", \"content\": \"You are a helpful and concise assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt_text\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens, # Note: 'max_tokens' is used here, not 'max_new_tokens'\n",
        "            top_p=top_p,\n",
        "            stream=False # For single response; set to True for streaming tokens\n",
        "        )\n",
        "\n",
        "        if completion.choices:\n",
        "            return completion.choices[0].message.content.strip()\n",
        "        else:\n",
        "            return \"No choices returned by the model.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Hugging Face API Error during generation: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return f\"An error occurred with Hugging Face: {type(e).__name__} - {e}\""
      ],
      "metadata": {
        "id": "j2KFbWgQYRNO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Application Logic (User Interaction) ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Welcome to the Hugging Face Text Completion App using {HF_MODEL_ID} via {HF_PROVIDER}!\")\n",
        "    print(\"Type 'quit' or 'exit' to end the session.\")\n",
        "    print(\"Type '!settings' to change AI model parameters (temperature, max_tokens, top_p).\")\n",
        "    print(\"\\nNOTE: The model ID is fixed to 'MiniMaxAI/MiniMax-M1-80k' for this setup.\")\n",
        "\n",
        "    # Initialize current parameters (model is fixed for this setup)\n",
        "    current_temperature = 0.7\n",
        "    current_max_tokens = 150\n",
        "    current_top_p = 1.0\n",
        "\n",
        "    def get_user_settings():\n",
        "        \"\"\"Function to prompt user for and validate AI parameters.\"\"\"\n",
        "        global current_temperature, current_max_tokens, current_top_p\n",
        "\n",
        "        print(\"\\n--- AI Model Settings (MiniMaxAI) ---\")\n",
        "        print(f\"Current Temperature: {current_temperature}\")\n",
        "        print(f\"Current Max Tokens: {current_max_tokens}\")\n",
        "        print(f\"Current Top P: {current_top_p}\")\n",
        "        print(\"Enter new values or press Enter to keep current value.\")\n",
        "\n",
        "        # Temperature\n",
        "        while True:\n",
        "            temp_input = input(f\"Enter Temperature (0.0 to 1.0, current: {current_temperature}): \").strip()\n",
        "            if not temp_input: break\n",
        "            try:\n",
        "                new_temp = float(temp_input)\n",
        "                if 0.0 <= new_temp <= 1.0: current_temperature = new_temp; break\n",
        "                else: print(\"Invalid range. Temperature must be between 0.0 and 1.0.\")\n",
        "            except ValueError: print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "        # Max Tokens\n",
        "        while True:\n",
        "            max_tokens_input = input(f\"Enter Max Tokens (integer > 0, current: {current_max_tokens}): \").strip()\n",
        "            if not max_tokens_input: break\n",
        "            try:\n",
        "                new_max_tokens = int(max_tokens_input)\n",
        "                if new_max_tokens > 0: current_max_tokens = new_max_tokens; break\n",
        "                else: print(\"Invalid range. Max Tokens must be a positive integer.\")\n",
        "            except ValueError: print(\"Invalid input. Please enter an integer.\")\n",
        "\n",
        "        # Top P\n",
        "        while True:\n",
        "            top_p_input = input(f\"Enter Top P (0.0 to 1.0, current: {current_top_p}): \").strip()\n",
        "            if not top_p_input: break\n",
        "            try:\n",
        "                new_top_p = float(top_p_input)\n",
        "                if 0.0 <= new_top_p <= 1.0: current_top_p = new_top_p; break\n",
        "                else: print(\"Invalid range. Top P must be between 0.0 and 1.0.\")\n",
        "            except ValueError: print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "        print(\"\\nSettings updated!\")\n",
        "        print(f\"New Temperature: {current_temperature}\")\n",
        "        print(f\"New Max Tokens: {current_max_tokens}\")\n",
        "        print(f\"New Top P: {current_top_p}\")\n",
        "        print(\"------------------------\")\n",
        "\n",
        "    # Get initial settings from user\n",
        "    get_user_settings()\n",
        "\n",
        "    while True:\n",
        "        user_prompt = input(\"\\nEnter your prompt (or type '!settings' to change parameters): \")\n",
        "\n",
        "        if user_prompt.lower() in ['quit', 'exit']:\n",
        "            print(\"Exiting application. Goodbye!\")\n",
        "            break\n",
        "        elif user_prompt.lower() == '!settings':\n",
        "            get_user_settings()\n",
        "            continue\n",
        "        elif not user_prompt.strip():\n",
        "            print(\"Input cannot be empty. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        print(\"Generating response...\")\n",
        "        ai_response = get_deepseek_completion(\n",
        "            user_prompt,\n",
        "            temperature=current_temperature,\n",
        "            max_tokens=current_max_tokens,\n",
        "            top_p=current_top_p\n",
        "        )\n",
        "\n",
        "        if ai_response and not ai_response.startswith(\"An error occurred\"):\n",
        "            print(\"\\nAI Response:\")\n",
        "            print(ai_response)\n",
        "        else:\n",
        "            print(\"Failed to get a valid response from the AI.\")\n",
        "            print(f\"Error details: {ai_response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EODYa5vAec7e",
        "outputId": "bb8bd0c0-671d-436d-b613-7df58461702c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Hugging Face Text Completion App using deepseek-ai/DeepSeek-R1-0528 via together!\n",
            "Type 'quit' or 'exit' to end the session.\n",
            "Type '!settings' to change AI model parameters (temperature, max_tokens, top_p).\n",
            "\n",
            "NOTE: The model ID is fixed to 'MiniMaxAI/MiniMax-M1-80k' for this setup.\n",
            "\n",
            "--- AI Model Settings (MiniMaxAI) ---\n",
            "Current Temperature: 0.7\n",
            "Current Max Tokens: 150\n",
            "Current Top P: 1.0\n",
            "Enter new values or press Enter to keep current value.\n",
            "Enter Temperature (0.0 to 1.0, current: 0.7): \n",
            "Enter Max Tokens (integer > 0, current: 150): \n",
            "Enter Top P (0.0 to 1.0, current: 1.0): \n",
            "\n",
            "Settings updated!\n",
            "New Temperature: 0.7\n",
            "New Max Tokens: 150\n",
            "New Top P: 1.0\n",
            "------------------------\n",
            "\n",
            "Enter your prompt (or type '!settings' to change parameters): Explain recursion like Iâ€™m five.\n",
            "Generating response...\n",
            "\n",
            "AI Response:\n",
            "<think>\n",
            "We are going to explain recursion to a five-year-old. We need to use a very simple analogy that a child can relate to.\n",
            "\n",
            "Idea: Use a story or a task that repeats itself, like stacking blocks or a Russian nesting doll.\n",
            "\n",
            "Let's go with the Russian nesting doll (Matryoshka doll). Each doll opens up to reveal a smaller doll inside, and this goes on until we get to the smallest doll that doesn't open.\n",
            "\n",
            "Explanation:\n",
            "\n",
            "\"Recursion is like when you have a big doll. You open the big doll and inside there is a smaller doll. Then you open that smaller doll, and inside there's an even smaller doll. You keep opening dolls until you get to the smallest doll that can't\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vEMnRlgB6im"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}